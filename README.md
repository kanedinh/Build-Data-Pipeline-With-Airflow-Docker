# Data Pipeline with Airflow-Docker

ðŸš€ In this project, I built a data pipeline using the ETL technique. 

Collecting data from this [website](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) by downloading. Then upload it to staging area. Transform and load into data warehouse. 

Apache Airflow orchestrated the entire workflow, ensure smoothness from extraction to transformation. 

Containerized the project using Docker for seamless environment management and consistent deployment.

## Tool & Technologies used in this project

- **Data Orchestration**: Apache Airflow
- **Staging Area and Data Warehousing**: PostgreSQL
- **ETL**: Python-Pandas
- **Containerization & Deployment**: Docker

![Data Pipeline Diagram](./images/datapipeline.jpg)

### Project Structure

```
Build-Data-Pipeline-With-Airflow-Docker
â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â”œâ”€â”€ script.py
â”‚   â”œâ”€â”€ transform_dim_date.py
â”‚   â”œâ”€â”€ transform_dim_location.py
â”‚   â”œâ”€â”€ transform_dim_payment_type.py
â”‚   â”œâ”€â”€ transform_dim_ratecode.py
â”‚   â”œâ”€â”€ transform_dim_store_and_fwd_flag.py
â”‚   â”œâ”€â”€ transform_dim_vendor.py
â”‚   â””â”€â”€ transform_fact_yellow_taxi.py
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ data_dictionary.jpg
â”‚   â”œâ”€â”€ datapipeline.jpg
â”‚   â””â”€â”€ diagram.jpg
â”œâ”€â”€ plugins
â”‚   â””â”€â”€ postgres_operator.py
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

### Data

ðŸš€ About dataset:
![Data Dictionary](./images/data_dictionary.jpg)

ðŸš€ Data warehouse Diagram:
![Data Warehouse](./images/diagram.jpg)